{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3: Wrangle OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling with SQL\n",
    "For this project the I have downloaded the Dallas, TX, United states Open Street Map data to work with and the file was downloaded in .osm.bz2 format which was uncompressed to get .osm file. It was a 40.7 mb Compressed file when downloaded and was 602MB after Un-Compressing the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cityclubdallas.com/wp-content/uploads/revslider/home1/Dallas2.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://cityclubdallas.com/wp-content/uploads/revslider/home1/Dallas2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics Overview:\n",
    "\n",
    "    As I have been working with a very big city (Dallas, TX, USA) the data set is huge. So runing the code has been difficult at times as it takes a lot of Processing time and computing power to Execute the code. So, The code has been developed and tested on a Sample file of the Original data set which is comparitively very small and Easy to work with. So the statistics were given in detail for both the sample and Original File. \n",
    "    \n",
    "    Sample OSM Data statistics:\n",
    "        Size of the Dataset: 5014 KB\n",
    "        Nodes: 1787 KB\n",
    "        Nodes_Tags: 98 KB\n",
    "        Ways: 146 KB\n",
    "        Ways_Nodes: 573 KB\n",
    "        Ways_Tags: 599 KB\n",
    "        DB Size: 101 KB\n",
    "        Key Count: {'lower': 5973, 'lower_colon': 12872, 'problemchars': 1, 'rest': 354}\n",
    "        Users: 597\n",
    "        Tag Count: {'member': 452,\n",
    "                     'nd': 24704,\n",
    "                     'node': 21555,\n",
    "                     'osm': 1,\n",
    "                     'relation': 15,\n",
    "                     'tag': 19200,\n",
    "                     'way': 2453}\n",
    "        \n",
    "            \n",
    "    Dallas OSM Data statistics:\n",
    "        Size of the Dataset: 617456 KB\n",
    "        Nodes: 223303 KB\n",
    "        Nodes_Tags: 12392 KB\n",
    "        Ways: 18181 KB\n",
    "        Ways_Nodes: 72906 KB\n",
    "        Wasy_Tags: 73783 KB\n",
    "        DB Size: 16873 KB\n",
    "        Key Count:  {'lower': 751246, 'lower_colon': 1586381, 'other': 39285, 'problemchars': 11}\n",
    "        Users: 1802\n",
    "        Tag Count:  {'bounds': 1,\n",
    "                     'member': 27634,\n",
    "                     'nd': 3130074,\n",
    "                     'node': 2694280,\n",
    "                     'osm': 1,\n",
    "                     'relation': 1826,\n",
    "                     'tag': 2376923,\n",
    "                     'way': 306677}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "\n",
    "dallas = \"dallas_texas.osm\"\n",
    "source = \"C:\\Users\\PremMithilesh\\Desktop\\Udacity DSND\\P3\\Finale\\DallasOSM\"\n",
    "dallasOSM = os.path.join(source, dallas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for counting the Number of tags in the Dataset Programatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 27634,\n",
       " 'nd': 3130074,\n",
       " 'node': 2694280,\n",
       " 'osm': 1,\n",
       " 'relation': 1826,\n",
       " 'tag': 2376923,\n",
       " 'way': 306677}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tagCounter(dallas):\n",
    "    tags={}\n",
    "    \n",
    "    for event,elem in ET.iterparse(dallas):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag]+=1\n",
    "        else:\n",
    "            tags[elem.tag]=1\n",
    "    return tags\n",
    "tagCounter(dallas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts the Key types and groups them based on the Regular Expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 751246, 'lower_colon': 1586381, 'other': 39285, 'problemchars': 11}\n"
     ]
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$') \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(dallas):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(dallas):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "caluculateKeys = process_map(dallasOSM)\n",
    "pprint.pprint(caluculateKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts the unique number of users that Contributed to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring users\n",
    "def process_map(dallas):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(dallas):\n",
    "        for e in element:\n",
    "            if 'uid' in e.attrib:\n",
    "                users.add(e.attrib['uid'])\n",
    "    return users\n",
    "users = process_map(dallasOSM)\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we try to find the problematic streetnames using a regular expression and we created an Expected and a Mapping funtion.\n",
    "# If the Street types is not in the expected, It checks the mapping function and Corrects it based on the Key given to its mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Improving Street Names\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Avenue\", \"Boulevard\", \"Circle\", \"Drive\", \"East\", \"Expressway\", \"Freeway\", \"Highway\", \"Lane\", \"North\", \"Parkway\",\n",
    "           \"Road\", \"South\", \"Street\", \"Trail\"]\n",
    "\n",
    "mapping = { \"Av\": \"Avenue\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"BLVD\": \"Boulevard\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"blvd\": \"Boulevard\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"dr\": \"Drive\",\n",
    "            \" E\": \" East\",\n",
    "            \"Expy\": \"Expressway\",\n",
    "            \"Fwy\": \"Freway\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Hwy78\": \"Highway 78\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \" N\": \" North\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"pkwy\": \"Parkway\",\n",
    "            \"RD\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"rd\": \"Road\",\n",
    "            \" S\": \" South\",\n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"Trl\": \"Trail\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating and developing the expected and Mapping function for the whole data set wold take a lot of time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Auditing Street Names\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(dallas, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types\n",
    "st_types = audit(dallasOSM)\n",
    "# pprint.pprint(dict(st_types))\n",
    "# The print statement has been voided due to space contraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Function Checks the Streetname and updates the streetname according to the Mapping function by its key value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping, regex):\n",
    "    m = regex.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(regex, mapping[street_type], name)\n",
    "\n",
    "    return name\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping, street_type_re)\n",
    "#         print name, \"\t\t<as>\t\t\", better_name\n",
    "# The print statement has been voided due to space contraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is similar to the Streetname correction which is done above. Now we correct the Postalcodes using a similar Process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postal_code_re = re.compile(r'.*(\\d{5}(\\-\\d{4})?)$')\n",
    "# A List of Entire Postal code of the Dallas. \n",
    "expected_ps = [\"75201\", \"75202\", \"75203\", \"75204\", \"75205\", \"75206\",\n",
    "              \"75207\", \"75208\", \"75209\", \"75210\", \"75211\", \"75212\",\n",
    "              \"75214\", \"75215\", \"75216\", \"75217\", \"75218\", \"75219\",\n",
    "              \"75220\", \"75221\", \"75222\", \"75223\", \"75224\", \"75225\",\n",
    "              \"75226\", \"75227\", \"75228\", \"75229\", \"75230\", \"75231\",\n",
    "              \"75232\", \"75233\", \"75234\", \"75235\", \"75236\",\" 75237\",\n",
    "              \"75238\", \"75239\", \"75240\", \"75241\", \"75242\", \"75243\", \"75244\",\n",
    "              \"75245\", \"75246\", \"75247\", \"75248\", \"75249\", \"75250\", \"75251\",\n",
    "              \"75252\", \"75253\", \"75258\", \"75260\", \"75261\", \"75262\", \"75263\",\n",
    "              \"75264\", \"75265\", \"75266\", \"75267\", \"75270\", \"75275\", \"75277\",\n",
    "              \"75283\", \"75284\", \"75285\", \"75286\", \"75287\", \"75294\", \"75295\",\n",
    "              \"75301\", \"75303\", \"75310\", \"75312\", \"75313\", \"75315\", \"75320\",\n",
    "              \"75323\", \"75326\", \"75336\", \"75339\", \"75342\", \"75346\", \"75350\",\n",
    "              \"75353\", \"75354\", \"75355\", \"75356\", \"75357\", \"75359\", \"75360\",\n",
    "              \"75363\", \"75364\", \"75367\", \"75368\", \"75370\", \"75371\", \"75372\",\n",
    "              \"75373\", \"75374\", \"75376\", \"75378\", \"75379\", \"75380\", \"75381\",\n",
    "              \"75382\", \"75386\", \"75387\",\" 75388\", \"75389\", \"75390\", \"75391\",\n",
    "              \"75392\", \"75393\", \"75394\", \"75395\", \"75396\", \"75397\", \"75398\",\n",
    "              \"76101\", \"76102\", \"76103\", \"76104\", \"76105\", \"76106\", \"76107\", \"76108\",\n",
    "              \"76109\", \"76110\", \"76111\", \"76112\", \"76113\", \"76114\", \"76115\", \"76116\",\n",
    "              \"76118\", \"76119\", \"76120\", \"76121\", \"76122\", \"76123\", \"76124\", \"76126\",\n",
    "              \"76129\", \"76130\", \"76131\", \"76132\", \"76133\", \"76134\", \"76135\", \"76136\",\n",
    "              \"76137\", \"76140\", \"76147\", \"76148\", \"76150\", \"76155\", \"76161\", \"76162\",\n",
    "              \"76163\", \"76164\", \"76177\", \"76178\", \"76179\", \"76181\", \"76185\", \"76191\",\n",
    "              \"76192\", \"76193\", \"76195\", \"76196\", \"76197\", \"76198\", \"76199\"]\n",
    "\n",
    "postal_mapping = {  \"TX 75229 1\": \"75229\",\n",
    "                    \"TX 76248 1\": \"76248\",\n",
    "                    \"TX 76012 1\": \"76012\",\n",
    "                    \"TX 76013 1\": \"76013\",\n",
    "                    \"TX 75230 2\": \"75230\",\n",
    "                    \"TX 75226 3\": \"75226\",\n",
    "                    \"TX 76034 2\": \"76035\",\n",
    "                    \"TX 75071 1\": \"75071\",\n",
    "                    \"TX 75070 1\": \"75070\",\n",
    "                    \"TX 76262 1\": \"76262\",\n",
    "                    \"TX 75074 1\": \"75074\",\n",
    "                    \"75070-4501\": \"75070\",\n",
    "                    \"75080-3338\": \"75080\",\n",
    "                    \"75104-1204\": \"75104\",\n",
    "                    \"75104-2135\": \"75104\",\n",
    "                    \"75215-1816\": \"75215\",\n",
    "                    \"76010-1183\": \"76010\",\n",
    "                    \"76013-3804\": \"76013\",\n",
    "                    \"76028-4910\": \"76028\",\n",
    "                    \"76209-1540\": \"76209\",\n",
    "                    \"TX 75070\": \"75070\",\n",
    "                    \"TX 75071\": \"75071\",\n",
    "                    \"TX 75074\": \"75074\",\n",
    "                    \"TX 75226\": \"75226\",\n",
    "                    \"TX 75229\": \"75229\",\n",
    "                    \"TX 75230\": \"75230\",\n",
    "                    \"TX 76012\": \"76012\",\n",
    "                    \"TX 76013\": \"76013\",\n",
    "                    \"TX 76034\": \"76034\",\n",
    "                    \"TX 76248\": \"76248\",\n",
    "                    \"TX 76262\": \"76262\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Auditing postal codes\n",
    "def audit_postalcodes(postal_codes, postalcode):\n",
    "    m = postal_code_re.search(postalcode)\n",
    "    if m:\n",
    "        code_type = m.group()\n",
    "        if code_type not in expected_ps:\n",
    "            postal_codes[code_type].add(postalcode)\n",
    "\n",
    "def is_postalcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_pc(osmfile):\n",
    "    osm_file = open(dallas, \"r\")\n",
    "    postal_codes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postalcode(tag):\n",
    "                    audit_postalcodes(postal_codes, tag.attrib['v'])\n",
    "\n",
    "    return postal_codes\n",
    "postal_codes = audit_pc(dallasOSM)\n",
    "# pprint.pprint(dict(postal_codes))\n",
    "# The print statement has been voided due to space contraint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_postalcode(code, postal_mapping, regex):\n",
    "    m = regex.search(code)\n",
    "    if m:\n",
    "        code_type = m.group()\n",
    "        if code_type in postal_mapping:\n",
    "            code = re.sub(regex, postal_mapping[code_type], code)\n",
    "\n",
    "    return code\n",
    "\n",
    "for postal_code, ways in postal_codes.iteritems():\n",
    "    for code in ways:\n",
    "        better_code = update_postalcode(code, postal_mapping, postal_code_re)\n",
    "#         print code, \"\t\t<as>\t\t\", better_code\n",
    "# The print statement has been voided due to space contraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "OSM_PATH =\"dallas_texas.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def correct_k(k):\n",
    "    index=k.find(':')\n",
    "    typ=k[:index]\n",
    "    k=k[index+1:]    \n",
    "    return k,typ\n",
    "def modified_user_name(name):\n",
    "    list_Username=name.split(' ')\n",
    "    number_words=len(list_Username)\n",
    "    firstname=list_Username[0]\n",
    "    for i in range(1,number_words):\n",
    "        username_Modified=firstname+' '+list_Username[i]\n",
    "        firstname=username_Modified\n",
    "    return firstname\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag=='node':\n",
    "        for i in node_attr_fields:\n",
    "            if i=='user':\n",
    "                user_name=element.attrib[i]\n",
    "                user_name_corrected=modified_user_name(user_name)\n",
    "                node_attribs[i]=user_name_corrected\n",
    "            else:    \n",
    "                node_attribs[i]=element.attrib[i]\n",
    "            \n",
    "    if element.tag=='way':\n",
    "        for i in way_attr_fields:\n",
    "            if i=='user':\n",
    "                user_name=element.attrib[i]\n",
    "                user_name_corrected=modified_user_name(user_name)\n",
    "                way_attribs[i]=user_name_corrected\n",
    "            else:    \n",
    "                way_attribs[i]=element.attrib[i]\n",
    "        \n",
    "    for tag in element.iter(\"tag\"):\n",
    "        dic={}\n",
    "        attributes=tag.attrib\n",
    "        if problem_chars.search(tag.attrib['k']):\n",
    "            continue\n",
    "        \n",
    "        if element.tag=='node':\n",
    "            dic['id']=node_attribs['id']\n",
    "        else:\n",
    "            dic['id']=way_attribs['id']\n",
    "        \n",
    "        dic['value'] = attributes['v']\n",
    "\n",
    "        colon_k=LOWER_COLON.search(tag.attrib['k'])\n",
    "        if colon_k:\n",
    "#             print colon_k.group(0)\n",
    "#             print tag.attrib['k']\n",
    "# print statement have been voided due to space constraint.\n",
    "            if(tag.attrib['k'] == 'addr:street'):\n",
    "                #update the street name \n",
    "                dic['value'] = update_name(dic['value'], mapping, street_type_re)\n",
    "            dic['key'],dic['type']=correct_k(tag.attrib['v'])\n",
    "            \n",
    "            if(tag.attrib['k'] == \"addr:postcode\"):\n",
    "                # Update the postal code\n",
    "                dic['value'] = update_postalcode(dic['value'], postal_mapping, postal_code_re)\n",
    "            dic['key'],dic['type']=correct_k(tag.attrib['v'])\n",
    "        else:\n",
    "            dic['key']=attributes['k']\n",
    "            dic['type']='regular'\n",
    "        \n",
    "        #print dic\n",
    "        tags.append(dic)\n",
    "    \n",
    "    if element.tag=='way':\n",
    "        position=0\n",
    "        for nd in element.iter(\"nd\"):\n",
    "            way_node_dic={}\n",
    "            way_node_dic['id']=way_attribs['id']\n",
    "            way_node_dic['node_id']=nd.attrib['ref']\n",
    "            way_node_dic['position']=position\n",
    "            position = position + 1\n",
    "            way_nodes.append(way_node_dic)\n",
    "             \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "            \n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file,          codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file,          codecs.open(WAYS_PATH, 'wb') as ways_file,          codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file,          codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_PATH, validate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'mydb.db'    # name of the sqlite database file\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "conn.text_factory = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a cursor object\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping the existing tables\n",
    "cur.execute('DROP TABLE IF EXISTS nodes')\n",
    "cur.execute('DROP TABLE IF EXISTS nodes_tags')\n",
    "cur.execute('DROP TABLE IF EXISTS ways')\n",
    "cur.execute('DROP TABLE IF EXISTS ways_nodes')\n",
    "cur.execute('DROP TABLE IF EXISTS ways_tags')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('CREATE TABLE ways(id INTEGER, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TIMESTAMP)')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the csv file as a dictionary, format the data as a list of tuples:\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [( i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-11-08T01:05:30Z'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "i['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(306677,)]\n"
     ]
    }
   ],
   "source": [
    "# Verifying the ways tags using SQLite and cross checking with the tagCounter method from above to make sure the db was properly executed. \n",
    "cur.execute('SELECT COUNT(*) FROM ways')\n",
    "count=cur.fetchall()\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This query could be used to cross verify the code we ran abouve initially to check the accuracy of the code and db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('CREATE TABLE nodes(id INTEGER, lat INTEGER, lon INTEGER, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TIMESTAMP)')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the table, specifying the column names and data types:\n",
    "cur.execute('CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)')\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x794a180>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset) VALUES (?, ?, ?, ?, ?, ?, ?);\", to_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woodpeck_fixbot', 1102543), ('Stephen Sprunk', 162358), ('fmmute', 98810), ('TexasNHD', 88356), ('25or6to4', 63955), ('Chris Lawrence', 60243), ('brianboru', 57225), ('balrog-kun', 56481), ('Dami_Tn', 55426), ('DaveHansenTiger', 46565)]\n"
     ]
    }
   ],
   "source": [
    "# cur.execute('SELECT TOP 10 e.user, COUNT(*) FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways)e GROUP BY e.user ORDER BY 2 DESC')\n",
    "cur.execute('SELECT e.user, COUNT(*) FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways)e GROUP BY e.user ORDER BY 2 DESC LIMIT 10')\n",
    "top10Contributors=cur.fetchall()\n",
    "print top10Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above is a list of the top 10 contributors of the DallasOSM dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1794,)]\n"
     ]
    }
   ],
   "source": [
    "# No.of Unique users that contributed to the data set.\n",
    "cur.execute('SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;')\n",
    "uniqueusers=cur.fetchall()\n",
    "print uniqueusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)')\n",
    "conn.commit()\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'],i['value'], i['type']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x794a180>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(338379,)]\n"
     ]
    }
   ],
   "source": [
    "# Counting Nodes_Tags\n",
    "cur.execute('SELECT COUNT(*) FROM nodes_tags')\n",
    "countnt=cur.fetchall()\n",
    "print countnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges Encountered:\n",
    "\n",
    "    Improper labeling/Abbrevation techniques used on street names, postal codes and posibily every other attribute if checked. \n",
    "    Huge sized Data Set 602MB.\n",
    "    Takes a lot of time to Process the data.\n",
    "    Takes a lot of Processing power and my computer got un-resposive at times. \n",
    "    \n",
    "    \n",
    "Ideas/Suggestions for Improvement:\n",
    "\n",
    "    As I have had problems with In-consistent Labling of the Tags, I would like to suggest to create a Standarized Dictionary of Tags that could be used as Street names and Let the top contributors of every Region have access to modify these. Once developed the Dictionary could be colaborated with entries from all over the globe to prevent the effects of Regional/Multi-Lingual Factors. Implementiung and following a proper disipline in making contrubution to the OSM will definitely prevent or atleast reduce such errors.\n",
    "    Users should create a self-disiplinatory ethics to contribute to the data in a deciplined manner. \n",
    "    \n",
    "Problems with this Improvement:\n",
    "\n",
    "    It could be a hactic Job to develop the Standarised Dictory of the Tags on various parts of the globe as These may differ/vary based on Regional/Ligual Factors.\n",
    "    The priviliged user with acess might get naughty or Lazy such control in their hands.\n",
    "    Production Costs might get involved. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "    https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md\n",
    "    http://stackoverflow.com/a/4020598\n",
    "    https://discussions.udacity.com/t/creating-db-file-from-csv-files-with-non-ascii-unicode-characters/174958/7\n",
    "    http://wiki.openstreetmap.org/wiki/Dallas,_Texas\n",
    "    https://en.wikipedia.org/wiki/OpenStreetMap\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
